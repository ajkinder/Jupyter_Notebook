{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make('CartPole-v0') # creates an OpenAI Gym environment, an object you interact with to step through the game\n",
    "epsilon = .01                 # controls the exploration vs exploitation balance\n",
    "gamma = .99                   # reward discount factor\n",
    "tau = .995                    # controls how quickly we update the target network\n",
    "random.seed(666)              # seed the random number generator for reproducibility\n",
    "batch_size = 128              # how much to sample from the replay buffer at a time\n",
    "max_ep = 500                  # number of games to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super(Q, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.main(torch.FloatTensor(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=int(size))\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "\n",
    "    def sample(self, count):\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "\n",
    "        s_arr = torch.FloatTensor(np.array([arr[0] for arr in batch]))\n",
    "        a_arr = torch.FloatTensor(np.array([arr[1] for arr in batch]))\n",
    "        r_arr = torch.FloatTensor(np.array([arr[2] for arr in batch]))\n",
    "        s2_arr = torch.FloatTensor(np.array([arr[3] for arr in batch]))\n",
    "        m_arr = torch.FloatTensor(np.array([arr[4] for arr in batch]))\n",
    "\n",
    "        return s_arr, a_arr.unsqueeze(1), r_arr.unsqueeze(1), s2_arr, m_arr.unsqueeze(1)\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def store(self, s, a, r, s2, d):\n",
    "        def fix(x):\n",
    "            if not isinstance(x, np.ndarray): return np.array(x)\n",
    "            else: return x\n",
    "\n",
    "        data = [s, np.array(a,dtype=np.float64), r, s2, 1 - d]\n",
    "        transition = tuple(fix(x) for x in data)\n",
    "        self.len = min(self.len + 1, self.maxSize)\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(timestep):\n",
    "    ts = 0\n",
    "    while ts < timestep:\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            a = env.action_space.sample()\n",
    "            s2, r, done, _ = env.step(int(a))\n",
    "            rb.store(s, a, r, s2, done)\n",
    "            ts += 1\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                s = s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    s, a, r, s2, m = rb.sample(batch_size) # get a batch of states, actions, reward, next states and\n",
    "                                           # masks (1 for game over, else zero) from the replay buffer\n",
    "\n",
    "    with torch.no_grad(): # don't track gradients when you pass through the target network\n",
    "        max_next_q, _ = q_target(s2).max(dim=1, keepdim=True) # get the next state value from the target net\n",
    "        y = r + m * gamma * max_next_q # sum rewards with discount penalty to avoid infinite time horizons\n",
    "    \n",
    "    q_estimates = torch.gather(q(s), 1, a.long())\n",
    "    loss = torch.pow((q_estimates - y), 2).mean() # calulate loss\n",
    "\n",
    "    # update Q network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target Q networked with weighted avereging\n",
    "    for param, target_param in zip(q.parameters(), q_target.parameters()):\n",
    "        target_param.data = target_param.data*tau + param.data*(1-tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 10.0\n",
      "Episode 10 Reward: 10.0\n",
      "Episode 20 Reward: 10.0\n",
      "Episode 30 Reward: 10.0\n",
      "Episode 40 Reward: 10.0\n",
      "Episode 50 Reward: 11.0\n",
      "Episode 60 Reward: 9.0\n",
      "Episode 70 Reward: 150.0\n",
      "Episode 80 Reward: 200.0\n",
      "Episode 90 Reward: 200.0\n",
      "Episode 100 Reward: 200.0\n",
      "Episode 110 Reward: 157.0\n",
      "Episode 120 Reward: 200.0\n",
      "Episode 130 Reward: 176.0\n",
      "Episode 140 Reward: 189.0\n",
      "Episode 150 Reward: 200.0\n",
      "Episode 160 Reward: 134.0\n",
      "Episode 170 Reward: 118.0\n",
      "Episode 180 Reward: 183.0\n",
      "Episode 190 Reward: 175.0\n",
      "Episode 200 Reward: 136.0\n",
      "Episode 210 Reward: 200.0\n",
      "Episode 220 Reward: 169.0\n",
      "Episode 230 Reward: 127.0\n",
      "Episode 240 Reward: 200.0\n",
      "Episode 250 Reward: 200.0\n",
      "Episode 260 Reward: 200.0\n",
      "Episode 270 Reward: 200.0\n",
      "Episode 280 Reward: 200.0\n",
      "Episode 290 Reward: 200.0\n",
      "Episode 300 Reward: 200.0\n",
      "Episode 310 Reward: 173.0\n",
      "Episode 320 Reward: 169.0\n",
      "Episode 330 Reward: 200.0\n",
      "Episode 340 Reward: 200.0\n",
      "Episode 350 Reward: 200.0\n",
      "Episode 360 Reward: 200.0\n",
      "Episode 370 Reward: 200.0\n",
      "Episode 380 Reward: 200.0\n",
      "Episode 390 Reward: 200.0\n",
      "Episode 400 Reward: 200.0\n",
      "Episode 410 Reward: 200.0\n",
      "Episode 420 Reward: 200.0\n",
      "Episode 430 Reward: 200.0\n",
      "Episode 440 Reward: 200.0\n",
      "Episode 450 Reward: 200.0\n",
      "Episode 460 Reward: 200.0\n",
      "Episode 470 Reward: 200.0\n",
      "Episode 480 Reward: 200.0\n",
      "Episode 490 Reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "# init the Q network and target network, a copy that lags behind to smooth training\n",
    "q = Q(env)\n",
    "q_target = deepcopy(q)\n",
    "\n",
    "# init the optimizer, which uses the loss to update the weights of the network\n",
    "optimizer = torch.optim.Adam(q.parameters(), lr=1e-3)\n",
    "\n",
    "# init the replay buffer and do some exploration to fill it\n",
    "rb = ReplayBuffer(1e6)\n",
    "explore(10000) \n",
    "\n",
    "# training loop\n",
    "ep = 0\n",
    "while ep < max_ep: # loop through some number of episodes, aka complete games\n",
    "    s = env.reset() # reset the environment at the state of the game\n",
    "    ep_r = 0\n",
    "    while True: # loop through a game frame by frame\n",
    "        with torch.no_grad():\n",
    "            # epsilon greedy exploration\n",
    "            if random.random() < epsilon: # some portion of the time, pick a random action to explore\n",
    "                a = env.action_space.sample()\n",
    "            else: # the rest of the time, take the action recomended by the network\n",
    "                a = int(np.argmax(q(s)))\n",
    "        \n",
    "        # take a step in the environment and get the resulting next state, reward, and done boolean\n",
    "        s2, r, done, _ = env.step(int(a))\n",
    "        rb.store(s, a, r, s2, done) # store in the replay buffer\n",
    "        ep_r += r # update episode reward\n",
    "\n",
    "        \n",
    "        if done: # if a ga,e endsbreak the loop and begin again, \n",
    "            if ep % 10 == 0:\n",
    "                print(f\"Episode {ep} Reward: {ep_r}\")\n",
    "            ep += 1\n",
    "            break\n",
    "        else: # otherwise continue\n",
    "            s = s2\n",
    "\n",
    "        update() # compute loss and update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
